/**
 * æ¥µç°¡ AI å°è©±æœå‹™
 * éœ€æ±‚ï¼šåªä¿ç•™æœ€åŸºæœ¬ä¸²æµ/éä¸²æµé‚è¼¯ï¼Œä¸åš tokens å‹•æ…‹è¨ˆç®—ã€ä¸åŠ ä½¿ç”¨é‡æ—¥èªŒã€ä¸åŠ é¡å¤– guardã€‚
 * ä¸å†æ–°å¢ä»»ä½•ã€Œä¿®å¾©ç©ºç™½ã€æˆ–ã€Œå„ªåŒ–ã€ç›¸é—œæŒ‡ä»¤èˆ‡è¨»è§£ã€‚
 */

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

// ä¿ç•™æœ€å°å‹å‹åˆ¥ï¼Œåƒ…ç”¨æ–¼éä¸²æµæ¨¡å¼è§£æ
export interface OpenAIResponseChoiceDelta {
  content?: string
}
export interface OpenAIResponse {
  choices?: Array<{
    message?: ChatMessage
    delta?: OpenAIResponseChoiceDelta
  }>
}

// ä¸å†å¼·åˆ¶é™„åŠ  system promptï¼ˆä¾ä½¿ç”¨è€…åŸå§‹éœ€æ±‚ï¼šç§»é™¤ä¿®å¾©/å„ªåŒ–é™„åŠ æŒ‡ä»¤ï¼‰

/**
 * å‘¼å« OpenAI API é€²è¡Œå°è©±
 * @param messages å°è©±è¨Šæ¯åˆ—è¡¨ï¼ˆä¸å« system messageï¼‰
 * @param onChunk ä¸²æµå›å‚³çš„å›èª¿å‡½æ•¸ï¼ˆæ¯æ”¶åˆ°ä¸€æ®µæ–‡å­—å°±å‘¼å«ï¼‰
 * @returns AI å›æ‡‰çš„å®Œæ•´æ–‡å­—å…§å®¹
 */
export async function callOpenAI(
  messages: Array<{ role: 'user' | 'assistant'; content: string }>,
  onChunk?: (chunk: string) => void
): Promise<{ content: string }> {
  // åƒ…ä¿ç•™æœ€è¿‘å°‘é‡è¨Šæ¯ï¼ˆé¿å…ç„¡é™å¢é•·ï¼‰
  const recent = messages.slice(-6)

  // å„ªå…ˆä½¿ç”¨ Vercel serverless APIï¼Œä¸å†ç›´é€£ OpenAI
  const envAny = (import.meta as any).env || {}
  
  // æ ¹æ“šç’°å¢ƒè‡ªå‹•é¸æ“‡ API ç«¯é»
  let upstreamUrl: string
  if (window.location.hostname.includes('vercel.app')) {
    upstreamUrl = '/api/openai-proxy'
  } else if (window.location.hostname.includes('github.io')) {
    upstreamUrl = 'https://maihouses.vercel.app/api/openai-proxy'
  } else {
    // æœ¬åœ°é–‹ç™¼ï¼šé è¨­ç”¨ Vercel
    upstreamUrl = envAny.VITE_AI_PROXY_URL || 'https://maihouses.vercel.app/api/openai-proxy'
  }

  const headers: Record<string, string> = {
    'Content-Type': 'application/json',
    // æç¤ºä»£ç†æˆ–ä¸Šæ¸¸æœå‹™ä»¥ç¹é«”ä¸­æ–‡è™•ç†
    'Accept-Language': 'zh-Hant-TW'
  }
  // ä¸å†éœ€è¦ Authorization headerï¼Œç”± Vercel serverless è™•ç†

  // åœ¨æœ€å‰é¢æ’å…¥ system promptï¼šçµ±ä¸€ç¹é«”ä¸­æ–‡ï¼Œä¸è¼¸å‡ºç°¡é«”
  const systemPrompt = {
    role: 'system' as const,
    content:
      'ä½ æ˜¯æˆ¿ç”¢è«®è©¢åŠ©ç†ã€‚ç„¡è«–ä½¿ç”¨è€…è¼¸å…¥ä½•ç¨®èªè¨€ï¼Œå›è¦†ä¸€å¾‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªã€æ¨™é»èˆ‡å­—å½¢ï¼Œé¿å…ç°¡é«”å­—ï¼‰ã€‚è‹¥éœ€è¼¸å‡ºåœ°é»ã€åƒ¹éŒ¢æˆ–é¢ç©ï¼Œè«‹ç”¨åœ¨åœ°å¸¸ç”¨æ ¼å¼ã€‚'
  }

  const bodyPayload = {
    model: 'gpt-4o-mini',
    messages: [
      systemPrompt,
      ...recent.map(m => ({ role: m.role, content: m.content }))
    ],
    stream: !!onChunk  // å¦‚æœæœ‰ onChunk å›èª¿å°±å•Ÿç”¨ä¸²æµ
  }

  const resp = await fetch(upstreamUrl, {
    method: 'POST',
    headers,
    body: JSON.stringify(bodyPayload)
  })

  // è‹¥ä¸Šæ¸¸é 2xxï¼Œç›´æ¥ä¸Ÿå‡ºéŒ¯èª¤è®“å‘¼å«ç«¯è™•ç†
  if (!resp.ok) {
    throw new Error(`Upstream error: ${resp.status}`)
  }

  // ä¸²æµæ¨¡å¼ï¼šåƒ…åœ¨ SSE å›æ‡‰æ™‚å•Ÿç”¨
  const isEventStream = resp.headers.get('content-type')?.includes('text/event-stream')
  if (onChunk && resp.body && isEventStream) {
    const reader = resp.body.getReader()
    const decoder = new TextDecoder()
    let acc = ''
    while (true) {
      const { done, value } = await reader.read()
      if (done) break
      const chunk = decoder.decode(value)
      for (const line of chunk.split('\n')) {
        if (!line.startsWith('data: ')) continue
        const data = line.slice(6).trim()
        if (!data || data === '[DONE]') continue
        try {
          const parsed = JSON.parse(data)
          const piece: string | undefined = parsed.choices?.[0]?.delta?.content
          if (piece) {
            acc += piece
            onChunk(piece)
          }
        } catch (_) {
          // å¿½ç•¥è§£æéŒ¯èª¤
        }
      }
    }
    return { content: acc }
  }

  // éä¸²æµï¼šå˜—è©¦è§£æ JSONï¼Œå¤±æ•—å‰‡å›å‚³ç©ºå­—ä¸²
  let text = ''
  try {
    const data: OpenAIResponse = await resp.json()
    console.log('ğŸ”µ OpenAI å®Œæ•´å›æ‡‰:', data)
    text = data?.choices?.[0]?.message?.content || ''
    console.log('ğŸ”µ æå–çš„æ–‡å­—å…§å®¹:', text)
  } catch (_) {
    text = ''
  }
  return { content: text }
}
