/**
 * OpenAI / LLM å°è©±æœå‹™ï¼ˆå‰ç«¯ï¼‰
 * é‡è¦ï¼šä¸å†æ–¼å‰ç«¯æ‰“åŒ…ä»»ä½• API Keyï¼Œå…¨éƒ¨è«‹é€éå¾Œç«¯ / proxy è½‰ç™¼ã€‚
 * ä½ å·²ç¶“æœ‰ Cloudflare Workers `mai-ai-proxy`ï¼Œæ­¤æª”æ¡ˆæœƒå„ªå…ˆä½¿ç”¨ proxyã€‚
 */

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

export interface OpenAIResponse {
  id: string
  object: string
  created: number
  model: string
  choices: Array<{
    index: number
    message: ChatMessage
    finish_reason: string
  }>
  usage: {
    prompt_tokens: number
    completion_tokens: number
    total_tokens: number
  }
}

/**
 * é‚æˆ¿å­/é‚é„°å±… AI åŠ©ç†çš„ç³»çµ±æç¤ºè©
 * æ¥µç°¡ç‰ˆ - åªä¿ç•™èªæ°£èˆ‡åŸå‰‡
 */
const SYSTEM_PROMPT = `ç¹ä¸­å›ç­”ï¼Œå£å»è‡ªç„¶å‹å–„ï¼›å…ˆçµ¦çµè«–ï¼Œ50â€“150 å­—å…§ã€æœ€å¤š 2 é»ã€å¿…è¦å†å• 1 é¡Œã€‚

ä½ æ˜¯é‚æˆ¿å­ AI åŠ©ç†ã€‚é‚é„°å±…æŸ¥ç¤¾å€å£ç¢‘ã€é‚æˆ¿å­å®‰å¿ƒé™ªè·‘å…¨ç¨‹ç•™ç—•ã€‚`

/**
 * å‘¼å« OpenAI API é€²è¡Œå°è©±
 * @param messages å°è©±è¨Šæ¯åˆ—è¡¨ï¼ˆä¸å« system messageï¼‰
 * @param onChunk ä¸²æµå›å‚³çš„å›èª¿å‡½æ•¸ï¼ˆæ¯æ”¶åˆ°ä¸€æ®µæ–‡å­—å°±å‘¼å«ï¼‰
 * @returns AI å›æ‡‰çš„å®Œæ•´æ–‡å­—å…§å®¹
 */
export async function callOpenAI(
  messages: Array<{ role: 'user' | 'assistant'; content: string }>,
  onChunk?: (chunk: string) => void
): Promise<{ content: string; usage?: { promptTokens: number; completionTokens: number; totalTokens: number } }> {
  /**
   * Proxy è¨­å®šï¼š
   * - æ¨è–¦è¨­ç½® VITE_AI_PROXY_URL æŒ‡å‘ä½ çš„ Cloudflare Workerï¼Œä¾‹å¦‚ï¼š
   *   https://mai-ai-proxy.your-account.workers.dev/api/chat
   * - è‹¥ç’°å¢ƒè®Šæ•¸ä¸å­˜åœ¨ï¼Œé€€å›ç›¸å°è·¯å¾‘ /api/chatï¼ˆå¯ç”±åå‘ä»£ç†æˆ–åŒç¶²åŸŸ worker è™•ç†ï¼‰
   * - ä¸å†ç›´æ¥å‘¼å«å®˜æ–¹ OpenAI APIï¼Œé¿å…å°‡é‡‘é‘°æ‰“é€² bundle è¢« Secret Scanning æ””æˆªã€‚
   */
  const proxyUrl = (import.meta as any).env?.VITE_AI_PROXY_URL || '/api/chat'

  // é™åˆ¶å°è©±æ­·å²é•·åº¦ï¼ˆåªä¿ç•™æœ€è¿‘ 8 è¼ªï¼Œæ¸›å°‘ tokens æ¶ˆè€—ï¼‰
  const recentMessages = messages.slice(-8)
  
  // è¨ˆç®—ä½¿ç”¨è€…æœ€å¾Œä¸€å‰‡è¨Šæ¯çš„ tokensï¼ˆç²—ä¼°ï¼šä¸­æ–‡ 1 å­— â‰ˆ 1.5 tokensï¼‰
  const lastUserMsg = recentMessages.filter(m => m.role === 'user').slice(-1)[0]
  const userTokensEstimate = lastUserMsg ? Math.ceil(lastUserMsg.content.length * 1.5) : 20
  
  // å‹•æ…‹ max_tokensï¼šæœ€ä½ 120ã€æœ€é«˜ 220ã€ä¸€èˆ¬ 150-200
  const maxTokens = Math.min(220, Math.max(120, Math.ceil(userTokensEstimate * 0.8)))
  
  // çµ„åˆå®Œæ•´è¨Šæ¯ï¼ˆsystem + æœ€è¿‘å°è©±æ­·å²ï¼‰
  const fullMessages: ChatMessage[] = [
    { role: 'system', content: SYSTEM_PROMPT },
    ...recentMessages.map(msg => ({
      role: msg.role,
      content: msg.content
    }))
  ]

  try {
    const response = await fetch(proxyUrl, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        messages: fullMessages,
        temperature: 0.6,
        // è®“å¾Œç«¯çµ±ä¸€é¸æ¨¡å‹èˆ‡ streamï¼Œé¿å…å‰ç«¯ç¡¬ç·¨
        max_tokens: maxTokens,
        stream: !!onChunk
      })
    })

    // å…ˆæª¢æŸ¥å›æ‡‰ç‹€æ…‹ï¼Œé¿å…åœ¨ 404/500 æ™‚èµ°åˆ°ä¸²æµåˆ†æ”¯é€ æˆç©ºç™½å…§å®¹
    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}))
      throw new Error(`AI Proxy éŒ¯èª¤: ${response.status} - ${JSON.stringify(errorData)}`)
    }

    // ä¸²æµæ¨¡å¼
    if (onChunk && response.body) {
      const reader = response.body.getReader()
      const decoder = new TextDecoder()
      let fullContent = ''

      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        const chunk = decoder.decode(value)
        const lines = chunk.split('\n').filter(line => line.trim() !== '')

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6)
            if (data === '[DONE]') continue

            try {
              const parsed = JSON.parse(data)
              const content = parsed.choices?.[0]?.delta?.content
              if (content) {
                fullContent += content
                onChunk(content)  // å³æ™‚å›å‚³æ¯å€‹ç‰‡æ®µ
              }
            } catch (e) {
              // å¿½ç•¥è§£æéŒ¯èª¤
            }
          }
        }
      }

      return { content: fullContent }
    }

    // éä¸²æµæ¨¡å¼ï¼ˆåŸæœ¬çš„é‚è¼¯ï¼‰
    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}))
      throw new Error(`AI Proxy éŒ¯èª¤: ${response.status} - ${JSON.stringify(errorData)}`)
    }

    // å¾Œç«¯ç›´æ¥ pass-through OpenAI å›æ‡‰ï¼›è‹¥ä¸æ˜¯åŸç”Ÿæ ¼å¼å‰‡å®¹éŒ¯
    const data: OpenAIResponse | any = await response.json()

    if (!data || !data.choices) {
      // proxy å¯èƒ½åªå› {content:"..."}
      const content = data?.content || ''
      return { content }
    }

    // è¨˜éŒ„ tokens ä½¿ç”¨æƒ…æ³ï¼ˆé–‹ç™¼ç’°å¢ƒï¼‰
    if (data.usage && import.meta.env.DEV) {
      console.log('ğŸ“Š Tokens ä½¿ç”¨:', {
        prompt: data.usage.prompt_tokens,
        completion: data.usage.completion_tokens,
        total: data.usage.total_tokens,
        max_tokens: maxTokens,
        ä¼°ç®—æˆæœ¬: `$${(data.usage.total_tokens * 0.00015 / 1000).toFixed(6)}`
      })
    }

    const messageContent = data.choices[0]?.message.content ?? ''
    
    const result: { content: string; usage?: { promptTokens: number; completionTokens: number; totalTokens: number } } = {
      content: messageContent
    }
    
    if (data.usage) {
      result.usage = {
        promptTokens: data.usage.prompt_tokens,
        completionTokens: data.usage.completion_tokens,
        totalTokens: data.usage.total_tokens
      }
    }
    
    return result

  } catch (error) {
    console.error('AI Proxy å‘¼å«å¤±æ•—:', error)
    throw error
  }
}
